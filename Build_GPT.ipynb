{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uxlao9p1Ycwz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/Build_Gpt/tiny_story.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "len(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KR1aWPktZ01Q",
        "outputId": "6e3b7031-6289-464c-d588-3cc99204f012"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5181596"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 64 # how many independent sequences will we process in parallel?\n",
        "block_size = 128 # what is the maximum context length for predictions?\n",
        "max_iters = 15000\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 256\n",
        "n_head = 8\n",
        "n_layer = 8\n",
        "dropout = 0.1\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('/content/drive/MyDrive/Build_Gpt/tiny_story.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "print(\"len(train_data) - \", len(train_data))\n",
        "print(\"len(val_data) - \", len(val_data))\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        tok_emb = self.token_embedding_table(idx)  #(B, T, n_embd(32))\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))   # (T, n_embd)\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)   # (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B,T,C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQ8-HPFea-3s",
        "outputId": "0899ce38-9a58-4e5d-95e6-4666d8daef95"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(train_data) -  4663436\n",
            "len(val_data) -  518160\n",
            "6.391899 M parameters\n",
            "step 0: train loss 4.6934, val loss 4.6924\n",
            "step 500: train loss 2.1615, val loss 2.1527\n",
            "step 1000: train loss 1.7376, val loss 1.7212\n",
            "step 1500: train loss 1.4863, val loss 1.4723\n",
            "step 2000: train loss 1.3450, val loss 1.3356\n",
            "step 2500: train loss 1.2628, val loss 1.2483\n",
            "step 3000: train loss 1.1904, val loss 1.1696\n",
            "step 3500: train loss 1.1382, val loss 1.1313\n",
            "step 4000: train loss 1.1027, val loss 1.0928\n",
            "step 4500: train loss 1.0708, val loss 1.0613\n",
            "step 5000: train loss 1.0427, val loss 1.0322\n",
            "step 5500: train loss 1.0162, val loss 1.0128\n",
            "step 6000: train loss 0.9969, val loss 0.9911\n",
            "step 6500: train loss 0.9777, val loss 0.9734\n",
            "step 7000: train loss 0.9669, val loss 0.9559\n",
            "step 7500: train loss 0.9530, val loss 0.9413\n",
            "step 8000: train loss 0.9381, val loss 0.9344\n",
            "step 8500: train loss 0.9281, val loss 0.9210\n",
            "step 9000: train loss 0.9220, val loss 0.9147\n",
            "step 9500: train loss 0.9072, val loss 0.9043\n",
            "step 10000: train loss 0.8974, val loss 0.8926\n",
            "step 10500: train loss 0.8867, val loss 0.8871\n",
            "step 11000: train loss 0.8820, val loss 0.8856\n",
            "step 11500: train loss 0.8706, val loss 0.8754\n",
            "step 12000: train loss 0.8702, val loss 0.8702\n",
            "step 12500: train loss 0.8600, val loss 0.8634\n",
            "step 13000: train loss 0.8553, val loss 0.8593\n",
            "step 13500: train loss 0.8518, val loss 0.8532\n",
            "step 14000: train loss 0.8478, val loss 0.8493\n",
            "step 14500: train loss 0.8388, val loss 0.8441\n",
            "step 14999: train loss 0.8352, val loss 0.8410\n",
            "\n",
            "\n",
            "\n",
            "Story Start\n",
            "\n",
            "One day, Jane went to find her swimming in the garden far away. He told Susie to the table and taste a not all things. They wanted to learn with each other.\n",
            "\n",
            "\"Look, little puppy?\" a Big replaced his could help. \n",
            "\n",
            "The man got the corner able a big paper on a year obeiem. Lily wanted to play on the foren, a kidding by their catent all. You believed you too. But you're help. Do you hlik the heal thing? You are talking homething you sure my dissful.\" They ran out of the sun, smiled fo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yPx0rs3EpI2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Do Generation"
      ],
      "metadata": {
        "id": "O0oFXgJbqJOq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0HRwZgHkvc0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "starting_text = encode(\"Once upon a time\")\n",
        "starting_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rq6S8W1lq5fi",
        "outputId": "96c50c6f-d2ef-413f-90a4-253e2283ca9f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[37, 62, 51, 53, 1, 69, 64, 63, 62, 1, 49, 1, 68, 57, 61, 53]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "starting_context = torch.tensor([starting_text], dtype=torch.long, device=device)\n",
        "starting_context"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AGKD20WrGLF",
        "outputId": "96ad3e63-40f5-440d-e9aa-6f6b18fa8c95"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[37, 62, 51, 53,  1, 69, 64, 63, 62,  1, 49,  1, 68, 57, 61, 53]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(starting_context, max_new_tokens=5000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhAS9csMr-x9",
        "outputId": "f6876164-2dd7-4447-e1c5-2dc52509cce8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time there was a mystery facs. It was a little girl named Lily. She liked to play in her dolls, but some fell. A deep of rice wind and determth a new grateful princess. One day, she saw a witch. She wanted to set the bush and went to the bird, something is the pink in the mapo. She saw her little fearly what 'Happening in the water!\" Mommy said, \"but always do not her!\"\n",
            "\n",
            "Jenny found a breat! She kept like so scaring bush interesting in the slide, \"Hellow, que. I'lp you're so taste. I did usaway it. I drove you to play a huge explore.\"\n",
            "\n",
            "The boy said, \"I can't take them!\" The voice felt like us. He was sad, but he didn't do.\n",
            " Sara saw something to get idea. Chose where that if the feel like he had.\n",
            "\n",
            "When they believed, Jake musure skiled and played selfice for his heart. Suddenly, the bird saw a big car with a ice. He was so happy he put the ball. He saw his mom tother to get scars and one more. \n",
            "\n",
            "\"That not is a myster!\" \n",
            "Mom took it afraid, Daisy and her mom asked her memoried. â€œEllie you bumped the petern in your toy, a man roll who is too. Wini you, Budy?â€ The smile said.\n",
            "\n",
            "The bear smiled and talked on the little pele. So she all played at the sky, understing why is a good. She want to give her a shill on water her flie.\n",
            "\n",
            "One day, Livily found an involaticly well home, Daisy! It was full way to reveal, a little bit near to the ground. The two frog there little girl saw a ireat with a menutes, fruit and tumble. The doctor was having lots of slowly and drove it.\n",
            "\n",
            "So one happened that with it tassle. They went to me and ran to their house margests to play. She took their home, she needed to came ix.\n",
            "\n",
            "But's owner too, the pilic couched. She took a duck and soft! Sam asked, Jake what happened. She helped Sto lead marble the ferforest. He likes it all meanched it bigger, but it was too late.\n",
            "\n",
            "The end. Tommy's sad shapes in and the baby sky. He started to go to tide upon a big and fell. He knew have some in the stick.\n",
            "\n",
            "Story end\n",
            "\n",
            "\n",
            "Story Start\n",
            "\n",
            "Once upon a time, in a small girl named Lily. She was the people on her way out to play with her friends and play. Every day, Lucy would not have to the light new and said, \"I will the one my me fishere.\" Max replied! \n",
            "\n",
            "His friends replied, \"The kind led let's glass. The buill did not near my diam.\" \n",
            "\n",
            "Then laughina as soon he could see a helprete berrot a power. The orange flamed with his friends and water his perm. \n",
            "\n",
            "As that apologized from the beach of the weire, feeling wonderful playing when he standing. They liked to harb to go very delicious!\n",
            "\n",
            "Lucy wasn't used he her and helped all out of bown this room. So she was proud of her car to children. She felt happy happy and didn't use something seets for here. She saw the paint animals into her.\n",
            "\n",
            "She showed her for a restroad. Her mom smiled; At replied, not finally went to them. Her enough was he had proteched the messy enging of home, and her small hurt blanket her friend.\n",
            "\n",
            "Jamethy pointed to get lost scared and wire happy, but Lucy nodded. One day, it was okay. Her mommy put he with a big bucket up on her garden. She put in the swars and stopped on a trail. Max was very happy and the air with an embarrassic too. \n",
            "\n",
            "Molly thought her pocket from then bed something happen. All again and there were very happy and a hutterfly white it cried. And they reached it, let too exend!\n",
            "\n",
            "Story end\n",
            "\n",
            "\n",
            "Story Start\n",
            "\n",
            "One day, there was a friendly kid named Lily. One day &h, Lily ate the park, while they were. Lily loved to play over the toy and eat Sam. One day, Lily went to the beach. She wasn't so loud until it if she does could. She said, \"We'll do a play you agame and join good! I want very frustrated it well!\" \n",
            "\n",
            "Holly and her mom opened her mom in a tiny room. Max had a rad the noise. Bella curioused all to the peach the pancake, duling that it had seen if it needed baking her.\n",
            "\n",
            "Later that day for a while. Mis give it from a home a numble in the stove. Lily, while she spent the sun and decided to see her mom. The sky felt about Little and the stretwor arms playing all day. She lived a girl new car with pride of you're. Andy wants to eat you.\"\n",
            "\n",
            "Lucy couldn't do. She realised she was working through the tree.\n",
            "\n",
            "\"Let's keep we chat!\" and said, the busy. \n",
            "\n",
            "There. They were done walk and wrottle paws. They watch them and passed them away. E. They were the hospiton and were so eating the pine!\n",
            "\n",
            "Story end\n",
            "\n",
            "\n",
            "Story Start\n",
            "\n",
            "Sara and Lily about press were two douncing at the base. There was a little voine. The parade in the park with hole. A had animal idea. The ocean the little time pushed it and wanted to make a loop. \n",
            "\n",
            "He understanding the frog leaved his pocket. He cryinched to a way to say sorve him. His mom me, sisy's ball dana were bunny and rot Grandma sad, but he was soon they saw that hole is funny beautiful! Have is the bird to help it!\n",
            "\n",
            "The people dressed be there. The little girl was able to explor a small tree with Sandy. With a big blue kite. It is obeden, but it live was a very hort. But then she saw a lot of touch a rock time. It i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "USIRGlw6sUUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(starting_context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXpY2TxEqILw",
        "outputId": "c2a4b036-3fd5-4680-a375-66b8eb3d01e6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Story Start\n",
            "\n",
            "One day, there was a little girl named Lily. She loved to eat pot of her pool and reads to look at the hairy.\n",
            "\n",
            "One day, her mommy said she was in the minutes with a big, making it nto play. Kitty was not dangeround he left right again and she started to sing. It not didn't wasn't too sweet.\n",
            "\n",
            "Tom that day, she went to the right bom. She told Zoomy and Max blaughed at her hair. He would repling and was so happy people hands one light. And the cleverful vheil had it!\n",
            "\n",
            "Suddenly, Jack would not lea\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SFbrGZdRqYb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SR8C0Se7qYKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pMVRxGbCqX9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6fXOS_sZbaPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the Model"
      ],
      "metadata": {
        "id": "rkFKszxfpKhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.save(m, '/content/drive/MyDrive/Build_Gpt/model_6M.pth')"
      ],
      "metadata": {
        "id": "0Ak1FZBTpL2H"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AkPQj0PP0xnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Model"
      ],
      "metadata": {
        "id": "LUnyA1ae0zA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = torch.load('/content/drive/MyDrive/Build_Gpt/model_6M.pth')"
      ],
      "metadata": {
        "id": "8-LNaSm1pnoN"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLRA3EQ7xN55",
        "outputId": "4ba4b898-bba7-456d-cc05-4fc72ebd5b07"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.391899 M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KPwv1Tgr08nN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}